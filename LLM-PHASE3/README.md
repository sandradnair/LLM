# ğŸ“š Custom Chatbot Q&A with Local LLMs (RAG Application)

This project is a **fully local, private Retrieval-Augmented Generation (RAG) application**.  
It allows you to upload your own documents and ask questions about them using a **Large Language Model (LLM) running entirely on your machine**.  

âœ… **100% Local & Private** â€“ No data ever leaves your system  
âœ… **No API Keys Needed** â€“ Uses open-source models via [Ollama](https://ollama.com)  
âœ… **Simple UI** â€“ Built with Streamlit for an easy user experience  
âœ… **PDF Document Support** â€“ Upload and query your PDFs  
âœ… **Docker-Free Setup** â€“ Simple installation without containers  

---

## âš¡ Core Technologies
- **Frontend (UI):** Streamlit  
- **LLM Serving:** Ollama (models like Llama 3, Mistral, etc.)  
- **Backend Orchestration:** LangChain  
- **Vector Database:** ChromaDB  
- **Document Loading:** PyPDFLoader  

---

## ğŸ”§ Local Setup and Installation

### 1. Prerequisites
- Python **3.8+**
- [Ollama](https://ollama.com) installed and running

---

### 2. Install Ollama & Download a Model

#### Install Ollama
- **macOS/Linux:**
  ```bash
  curl -fsSL https://ollama.com/install.sh | sh
Windows:
Download and run the installer from Ollama Downloads.

Pull a Model (recommended: llama3)
bash
Copy code
ollama pull llama3
â„¹ï¸ Ollama starts a background server automatically after installation.

3. Set Up the Project
Clone the repository:

bash
Copy code
git clone <your-repo-url>
cd custom-rag-app
Or manually create a folder custom-rag-app/ and add app.py + requirements.txt.

Create a Virtual Environment
bash
Copy code
python -m venv venv
Activate the environment:

macOS/Linux:

bash
Copy code
source venv/bin/activate
Windows:

bash
Copy code
venv\Scripts\activate
Install Dependencies
bash
Copy code
pip install -r requirements.txt
4. Run the Application
bash
Copy code
streamlit run app.py
Your browser will open at ğŸ‘‰ http://localhost:8501

ğŸš€ How to Use
Upload a PDF Document â€“ Select a file from the sidebar.

Process the Document â€“ Click the button to:

Load & split the PDF

Generate embeddings

Store them in ChromaDB

Ask a Question â€“ Type your query in the input box.

Get an Answer â€“ The app retrieves relevant chunks & generates an answer using your local LLM.

ğŸ“‚ Project Structure
bash
Copy code
custom-rag-app/
â”‚
â”œâ”€â”€ venv/              # Python virtual environment
â”œâ”€â”€ app.py             # Main Streamlit app
â”œâ”€â”€ requirements.txt   # Dependencies
â””â”€â”€ README.md          # Project documentation
ğŸ›  Troubleshooting
Ollama Connection Error
Ensure Ollama is running (desktop app open or ollama serve in a terminal).

Slow Performance
Try a lighter model (e.g., mistral) if response time is slow. Performance depends on your CPU/GPU.

streamlit command not found
Activate your virtual environment first:

bash
Copy code
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
ğŸ¯ Future Enhancements
Multi-file support

Advanced query refinement

Support for more document types (DOCX, TXT, etc.)